
# âš™ï¸ Consulta LLM - Backend (Node.js Express)

API backend para consultar uma LLM gratuita, manter contexto da conversa e responder mensagens do cliente.

---

## ğŸ“± Funcionalidades

- Recebe mensagens via POST
- Consulta LLM externa (ex: OpenAI)
- MantÃ©m contexto por sessÃ£o
- Responde com texto gerado pela LLM
- CORS habilitado para comunicaÃ§Ã£o com frontend

---

## ğŸš€ Tecnologias usadas

- [Node.js](https://nodejs.org/)
- [Express](https://expressjs.com/)
- [cors](https://github.com/expressjs/cors)
- [dotenv](https://github.com/motdotla/dotenv)
- Fetch API (node-fetch ou nativo)

---

## ğŸ“‚ Estrutura

```
.
â”œâ”€â”€ index.js
â”œâ”€â”€ package.json
â”œâ”€â”€ .env
```

---

## âš™ï¸ Como rodar

1. Instale as dependÃªncias:

```bash
npm install
```

2. Crie o arquivo `.env` com sua chave:

```
OPENAI_API_KEY=sua_chave_aqui
PORT=3000
```

3. Inicie o servidor:

```bash
node index.js
```

4. A API ficarÃ¡ disponÃ­vel em:

```
http://localhost:3000
```

---

## ğŸŒ Endpoints

### POST `/chat`

Recebe JSON:

```json
{
  "message": "texto da mensagem",
  "sessionId": "id_unico_sessao"
}
```

Retorna JSON:

```json
{
  "response": "texto gerado pela LLM"
}
```

---

## ğŸ“Œ ObservaÃ§Ãµes

- Ajuste o IP para acessar de dispositivos externos.
- Verifique firewall e permissÃµes de rede.
- Garanta que a chave da API estÃ¡ vÃ¡lida.

---

## ğŸ“ Suporte

DÃºvidas ou problemas? Abra uma issue ou entre em contato com o desenvolvedor.

---

Feito com â¤ï¸ por JosÃ© Fernando
